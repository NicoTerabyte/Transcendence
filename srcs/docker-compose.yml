services:
  proxy:
    image: proxy
    build:
      context: ./proxy
      dockerfile: Dockerfile
    container_name: proxy
    restart: unless-stopped
    ports:
      - "443:443"
    depends_on:
      grafana:
        condition: service_healthy
      prometheus:
        condition: service_healthy
      node-exporter:
        condition: service_healthy
      alertmanager:
        condition: service_healthy
      auth-service:
        condition: service_healthy
      history-service:
        condition: service_healthy
      tournament-service:
        condition: service_healthy
      #ELK
      kibana:
        condition: service_healthy
      es01:
        condition: service_healthy
      es02:
        condition: service_healthy
      es03:
        condition: service_healthy
    volumes:
      - ./media:/app/media:rw
      - ./proxy/logs:/var/log/nginx
      - ./frontend:/var/www/html # mount the static frontend directly to ngnix
      # - ./proxy/logs:/var/log/modsecurity
    healthcheck:
      test: ["CMD-SHELL", "curl -f -k https://localhost/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    networks:
      - internal

  nginx-prometheus-exporter:
    image: nginx/nginx-prometheus-exporter:1.4
    container_name: nginx-prometheus-exporter
    restart: unless-stopped
    depends_on:
      - proxy
    command: [
      "--nginx.scrape-uri", "http://proxy:8080/stub_status",
    ]
    networks:
      - internal

  auth-service:
    image: auth-service
    container_name: auth-service
    build:
      context: ./backend/auth_service
      dockerfile: Dockerfile
    command: bash -c "/app/start.sh"
    env_file:
      - .env
    restart: unless-stopped
    volumes:
      - ./media:/app/media:rw  # Shared volume with read/write access
    depends_on:
      auth_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8001/watchman/?skip=watchman.checks.storage || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - internal

  auth_db:
    image: postgres:15
    container_name: auth_db
    env_file: ".env"
    environment:
     - POSTGRES_USER=${POSTGRES_USER}
     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
     - POSTGRES_DB=${POSTGRES_AUTH_DB}
     - POSTGRES_HOST=${POSTGRES_AUTH_HOST}
    restart: always
    volumes:
      - postgres_auth:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal

  grafana:
    image: grafana/grafana:11.4.0
    container_name: grafana
    env_file: ".env"
    restart: unless-stopped
    environment:
      - GF_SERVER_ROOT_URL=https://localhost/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH="true"
    volumes:
      - ./monitoring/grafana/conf/grafana.ini:/etc/grafana/grafana.ini
      - ./monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/dashboards/django.json:/var/lib/grafana/dashboards/django.json
      - ./monitoring/grafana/dashboards/django2.json:/var/lib/grafana/dashboards/django2.json
      - ./monitoring/grafana/dashboards/node.json:/var/lib/grafana/dashboards/node.json
      # - ./grafana/logs:/var/log/grafana  # Mount host directory for logs
    depends_on:
      prometheus:
        condition: service_healthy
      grafana_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - internal

  grafana_db:
    image: postgres:15
    container_name: grafana_db
    env_file: ".env"
    environment:
     - POSTGRES_USER=${POSTGRES_USER}
     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
     - POSTGRES_DB=${POSTGRES_G_DB}
     - POSTGRES_HOST=${POSTGRES_G_HOST}
    restart: always
    volumes:
      - postgres_grafana:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal

  prometheus:
    image: prom/prometheus:v3.1.0
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus/rules_1.yml:/etc/prometheus/rules_1.yml
      - ./monitoring/prometheus/rules_2.yml:/etc/prometheus/rules_2.yml
      - ./monitoring/prometheus/rules_3.yml:/etc/prometheus/rules_3.yml
      - ./monitoring/prometheus/rules_4.yml:/etc/prometheus/rules_4.yml

      - prometheus_data:/prometheus/data
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=10d'  # Retain data for 10 days
    depends_on:
      auth-service:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:9090/-/healthy || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    networks:
      - internal


  node-exporter:
    image: prom/node-exporter:v1.8.2
    container_name: node-exporter
    restart: unless-stopped
    depends_on:
     - prometheus
    command:
      - '--no-collector.thermal_zone'
      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|run)($|/)'
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:9100 || exit 1"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 10s
    networks:
      - internal

  alertmanager:
    image: prom/alertmanager:v0.28.0
    container_name: alertmanager
    env_file: ".env"
    restart: unless-stopped
    depends_on:
     - prometheus
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - ./monitoring/alertmanager/smtp_password.txt:/etc/alertmanager/smtp_password.txt
      - alertmanager_data:/data
    command:
      - --config.file=/etc/alertmanager/alertmanager.yml
    healthcheck:
      test: ["CMD-SHELL", "wget --spider http://localhost:9093 || exit 1"]
      interval: 30s
      retries: 3
      start_period: 10s
      timeout: 10s
    networks:
      - internal

  postgres-exporter-grafana:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: postgres-exporter-grafana
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@grafana_db:5432/grafana_db?sslmode=disable"
    restart: unless-stopped
    command:
      - '--web.listen-address=:9188'
    depends_on:
      prometheus:
        condition: service_healthy
      grafana_db:
        condition: service_healthy
    networks:
      - internal

  postgres-exporter-auth-db:
    image: prometheuscommunity/postgres-exporter:v0.16.0
    container_name: postgres-exporter-auth-db
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@auth_db:5432/auth_db?sslmode=disable"
    restart: unless-stopped
    depends_on:
      prometheus:
        condition: service_healthy
      auth_db:
        condition: service_healthy
    networks:
      - internal

  adminer:
    image: adminer
    container_name: adminer
    restart: unless-stopped
    networks:
      - internal


  tournament-service:
    image: tournament-service
    container_name: tournament-service
    build:
      context: ./backend/tournament-service
      dockerfile: Dockerfile
    # command: bash -c "python manage.py makemigrations && python manage.py migrate && python manage.py runserver 0.0.0.0:8003"
    command: >
      bash -c "python manage.py makemigrations &&
               python manage.py migrate &&
               gunicorn tournament.wsgi:application --bind 0.0.0.0:8003 --workers=3"
    env_file:
      - .env
    restart: unless-stopped
    depends_on:
      tournament_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8003/watchman/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - internal

  tournament_db:
    image: postgres:15
    container_name: tournament_db
    env_file: ".env"
    environment:
     - POSTGRES_USER=${POSTGRES_USER}
     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
     - POSTGRES_DB=${POSTGRES_TOURNAMENT_DB}
     - POSTGRES_HOST=${POSTGRES_TOURNAMENT_HOST}
    restart: always
    volumes:
      - postgres_tournament:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal

  history-service:
    image: history-service
    container_name: history-service
    build:
      context: ./backend/history
      dockerfile: Dockerfile
    # command: bash -c "python manage.py makemigrations && python manage.py migrate && python manage.py runserver 0.0.0.0:8002"
    command: >
      bash -c "python manage.py makemigrations &&
               python manage.py migrate &&
               gunicorn history_service.wsgi:application --bind 0.0.0.0:8002 --workers=3"
    env_file:
      - .env
    restart: unless-stopped
    depends_on:
      history_db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8002/watchman/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - internal

  history_db:
    image: postgres:15
    container_name: history_db
    env_file: ".env"
    environment:
     - POSTGRES_USER=${POSTGRES_USER}
     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
     - POSTGRES_DB=${POSTGRES_HISTORY_DB}
     - POSTGRES_HOST=${POSTGRES_HISTORY_HOST}
    restart: always
    volumes:
      - postgres_history:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - internal

#ELK STACK
  setup:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    container_name: setup
    environment:
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - KIBANA_PASSWORD=${KIBANA_PASSWORD}
    command: /usr/share/elasticsearch/setup.sh
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - ./ELK/setup.sh:/usr/share/elasticsearch/setup.sh:ro
      - ./ELK/elasticsearch/config/elastic:/usr/share/elasticsearch/config/elastic
    user: "0"
    healthcheck:
      test: ["CMD-SHELL", "[ -f config/certs/es01/es01.crt ]"]
      interval: 1s
      timeout: 5s
      retries: 120
    networks:
      - internal

  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    container_name: es01
    labels:
      co.elastic.logs/module: elasticsearch
    environment:
      - network.host=0.0.0.0
      - ES_JAVA_OPTS=-Xmx512m -Xms512m
      - node.name=es01
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es02,es03
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es01/es01.key
      - xpack.security.http.ssl.certificate=certs/es01/es01.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es01/es01.key
      - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - es_data01:/usr/share/elasticsearch/data
    depends_on:
      setup:
        condition: service_healthy
    mem_limit: 2g
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s -u elastic:${ELASTIC_PASSWORD} --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -E 'my-cluster'",
          # "curl -s -u elastic:${ELASTIC_PASSWORD} --cacert config/certs/ca/ca.crt https://localhost:9200/_cat/health | grep -E 'green|yellow'"
          # "curl -s -u elastic:${ELASTIC_PASSWORD} --cacert config/certs/ca/ca.crt https://localhost:9200/_cluster/health | grep -E '\"status\":\"(green|yellow)'",
          # "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
      # start_period: 60s
    networks:
      - internal

  es02:
    depends_on:
      es01:
        condition: service_healthy
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    container_name: es02
    labels:
      co.elastic.logs/module: elasticsearch
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - es_data02:/usr/share/elasticsearch/data
    environment:
      - network.host=0.0.0.0
      - ES_JAVA_OPTS=-Xmx512m -Xms512m
      - node.name=es02
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es02,es03
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es02/es02.key
      - xpack.security.http.ssl.certificate=certs/es02/es02.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es02/es02.key
      - xpack.security.transport.ssl.certificate=certs/es02/es02.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    mem_limit: 2g
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          "CMD-SHELL",
            "curl -s -u elastic:${ELASTIC_PASSWORD} --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -E 'my-cluster'",
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    networks:
      - internal


  es03:
    depends_on:
      es01:
        condition: service_healthy
    image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
    container_name: es03
    labels:
      co.elastic.logs/module: elasticsearch
    volumes:
      - certs:/usr/share/elasticsearch/config/certs
      - es_data03:/usr/share/elasticsearch/data
    environment:
      - network.host=0.0.0.0
      - ES_JAVA_OPTS=-Xmx512m -Xms512m
      - node.name=es03
      - cluster.name=${CLUSTER_NAME}
      - cluster.initial_master_nodes=es01,es02,es03
      - discovery.seed_hosts=es01,es02,es03
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - bootstrap.memory_lock=true
      - xpack.security.enabled=true
      - xpack.security.http.ssl.enabled=true
      - xpack.security.http.ssl.key=certs/es03/es03.key
      - xpack.security.http.ssl.certificate=certs/es03/es03.crt
      - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.enabled=true
      - xpack.security.transport.ssl.key=certs/es03/es03.key
      - xpack.security.transport.ssl.certificate=certs/es03/es03.crt
      - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
      - xpack.security.transport.ssl.verification_mode=certificate
      - xpack.license.self_generated.type=${LICENSE}
    mem_limit: 2g
    ulimits:
      memlock:
        soft: -1
        hard: -1
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -s -u elastic:${ELASTIC_PASSWORD} --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -E 'my-cluster'",

        ]
      interval: 10s
      timeout: 10s
      retries: 120
    networks:
      - internal

  kibana:
    image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
    container_name: kibana
    labels:
      co.elastic.logs/module: kibana
    environment:
      - SERVERNAME=kibana
      - SERVER_PUBLICBASEURL=https://localhost/kibana
      - SERVER_REWRITEBASEPATH=true
      - ELASTICSEARCH_HOSTS=https://es01:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
      - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
      - XPACK_SECURITY_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_REPORTING_ENCRYPTIONKEY=${ENCRYPTION_KEY}
      - XPACK_REPORTING_ROLES_ENABLED=false
      - XPACK_REPORTING_KIBANASERVER_HOSTNAME=localhost
    volumes:
      - certs:/usr/share/kibana/config/certs
      - ./ELK/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml
      - kibana_data:/usr/share/kibana/data
    depends_on:
      es01:
        condition: service_healthy
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -f -k https://localhost:5601/kibana/api/status || exit 1"
          # "curl -s -I -k https://localhost:5601/kibana/api/status | grep 'HTTP/1.1 200 OK'"
        ]
      interval: 10s
      timeout: 10s
      retries: 120
    mem_limit: ${KB_MEM_LIMIT}
    networks:
      - internal

  logstash:
    image: docker.elastic.co/logstash/logstash:${STACK_VERSION}
    container_name: logstash
    labels:
      co.elastic.logs/module: logstash
    environment:
      - LS_JAVA_OPTS=-Xmx512m -Xms512m
      - NODE_NAME="logstash"
      - xpack.monitoring.enabled=false
      - ELASTIC_USER=elastic
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTIC_HOSTS=https://es01:9200
    command: logstash -f /usr/share/logstash/pipeline/logstash.conf
    user: root
    volumes:
      - ./ELK/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
      - ./ELK/logstash/logstash.yml:/usr/share/logstash/config/logstash.yml
      - ./proxy/logs:/var/log/nginx  # Mount Nginx logs to Logstash
      - logstash_data:/usr/share/logstash/data
      - certs:/usr/share/logstash/certs
    depends_on:
      es01:
        condition: service_healthy
      kibana:
        condition: service_healthy
    mem_limit: ${LS_MEM_LIMIT}
    networks:
      - internal

  filebeat:
    image: docker.elastic.co/beats/filebeat:${STACK_VERSION}
    container_name: filebeat
    labels:
      co.elastic.logs/module: filebeat
    environment:
      - container
    volumes:
      - ./ELK/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - /var/lib/docker/containers:/var/lib/docker/containers
      - /var/run/docker.sock:/var/run/docker.sock
      - ./proxy/logs:/var/log/nginx  # Mount logs
      - certs:/usr/share/filebeat/certs
    depends_on:
      - logstash
    networks:
      - internal


networks:
  internal:
    driver: bridge
  # monitoring:
  #   driver: bridge

volumes:
  postgres_auth:
  postgres_grafana:
  prometheus_data:
  alertmanager_data:
  postgres_tournament:
  postgres_history:
  #ELK
  certs:
  es_data01:
  es_data02:
  es_data03:
  kibana_data:
  logstash_data:
